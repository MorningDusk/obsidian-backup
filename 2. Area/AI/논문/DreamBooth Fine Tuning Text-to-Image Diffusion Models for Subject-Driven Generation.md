---
date: 2025-07-01
aliases:
  - "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation"
tags:
  - computer_vision
  - ai
  - article
  - text-to-image
  - diffusion
  - fine-tuning
link: https://arxiv.org/abs/2208.12242
연구 목적: 주어진 특정 주제(물체나 애완동물 등)의 몇 장(3-5장)의 이미지만으로 텍스트-이미지 확산 모델을 미세 조정하여 해당 주제의 고유 특성을 유지하면서 다양한 맥락과 상황에서 새로운 이미지를 생성하는 기술 개발
연구 방법: 특정 주제를 고유 식별자와 연결하여 확산 모델의 어휘에 삽입하고, 언어 표류(language drift)를 방지하기 위한 자생적 클래스 특정 사전 보존 손실(autogenous class-specific prior preservation loss)을 도입.
결과 변수: 주제 충실도(subject fidelity), 프롬프트 충실도(prompt fidelity), DINO 및 CLIP-I 메트릭, 다양성 측정
주요 결과: 제안된 DreamBooth 방법은 기존 접근법(Textual Inversion)보다 주제와 프롬프트 충실도가 높으며, 주제의 핵심 시각적 특징을 유지하면서 재상황화, 속성 수정, 예술적 렌더링, 시점 합성 등 다양한 응용이 가능함을 보여줌
---
# Introduction
## 연구 배경과 동기
최근 몇 년간 text-to-image 생성 분야에서 혁신적인 발전이 있었다. DALL-E2, Imagen, Stable Diffusion과 같은 대규모 text-to-image 모델들은 자연어로 작성된 텍스트 프롬프트를 기반으로 고품질의 다양한 이미지를 합성할 수 있게 되었다. 이러한 모델들의 핵심 장점 중 하나는 대규모 image-caption 쌍 데이터에서 학습한 강력한 의미적 사전 지식(semantic prior)이다.
예를 들어, 이러한 사전 지식은 "개"라는 단어를 다양한 포즈와 상황에서 나타날 수 있는 여러 개체 인스턴스와 연결하는 방법을 학습한다. 하지만 이런 놀라운 학습 능력에도 불구하고, 현재 모델들은 중요한 한계를 가지고 있다. 바로 주어진 참조 이미지 세트에서 특정 주제의 외형을 정확히 모방하고, 다른 맥락에서 동일한 주제의 새로운 표현을 합성하는 능력이 부족하다는 것이다.
## 기존 방법의 근본적인 한계
이러한 한계의 주된 이유는 모델의 출력 도메인 표현력이 제한되어 있기 때문이다. 아무리 상세한 텍스트 설명을 제공해도 외형이 다른 인스턴스들이 생성될 수 있다. 심지어 CLIP과 같은 language-vision 공간에 텍스트 임베딩이 있는 모델들도 주어진 대상이 외형을 정확히 재구성할 수 없고, 단지 이미지 콘텐츠의 변형만을 생성할 수 있다.
예를 들어, "내가 기르는 특별한 얼룩무늬를 가진 강아지가 에펠탑 앞에서 산책하는 모습"을 생성하고 싶다면, 기존 모델들로는 그 특별한 얼룩무늬의 정확한 패턴이나 강아지의 고유한 특징들을 보존하면서 새로운 상황에 배치하기가 거의 불가능했다.
![[Pasted image 20250701101612.png]]
## DreamBooth의 혁신적 접근법
본 연구에서는 text-to-image diffusion model의 "개인화(personalization)"를 위한 완전히 새로운 접근 방식을 제시한다. 핵심 목표는 사용자가 생성하고 싶어하는 특정 주제와 새로운 고유 단어를 연결하도록 모델의 language-vision 딕셔너리를 확장하는 것이다.
이 새로운 딕셔너리가 모델에 내장되면, 이러한 고유 단어들을 사용하여 주요 식별 특징을 보존하면서 다양한 장면에서 상황에 맞는 주제의 새로운 사실적 이미지를 합성할 수 있다. 이 효과는 마치 "magic photo booth"과 같다. 몇 장의 사진을 찍으면, 이 사진관은 간단하고 직관적인 텍스트 프롬프트의 안내에 따라 다양한 조건과 장면에서 대상의 이미지를 생성한다.
![[Pasted image 20250701102321.png]]
## 구체적인 연구 목표와 기여
본 연구의 구체적인 목표는 피사체의 이미지 몇 개(대략 3-5장)가 주어졌을 때, 고유한 식별자로 합성할 수 있다록 모델의 출력 도메인에 해당 피사체를 이식하는 것이다. 이를 달성하기 위해 두 가지 주요 기술적 기여를 제시한다:
1. **Subject-driven generation**이라는 새로운 문제 설정을 정의
2. 피사체 클래스에 대한 모델의 의미적 지식을 보존하면서 few-shot setting에서 text-to-image diffusion model을 미세 조정하는 새로운 기술을 개발
# Related Work (관련 연구)
## 기존 이미지 합성 기술들의 한계
### 이미지 합성(Image Composition) 기술들
기존의 이미지 합성 기술들은 주어진  주제를 새로운 배경에 복제하여 장면에 자연스럽게 융합시키는 것을 목표로 했다. 하지만 이러한 방법들은 여러 심각한 제약이 있었다. 첫째, 새로운 포즈를 고려하면 보통 3D 재구성 기술이 필요한데, 이를 일반적으로 강체 물체에서만 작동하고 더 많은 시점이 필요했다. 둘째, 장면 통합 문제(조명, 그림자, 접촉)와 새로운 장면 생성 불가능이라는 한계가 있었다.
### Text-to-Image 편집 및 합성
텍스트 기반 이미지 조작은 GAN과 CLIP 같은 image-text 표현을 결합하여 상당한 진전을 이뤘다. 하지만 이러한 방법들은 구조화된 시나리오(예: 인간 얼굴 편집)에서는 잘 작동하지만, 주제가 다양한 데이터셋에서는 어려움을 겪었다. 또한 대부분의 편집 접근법은 전역 속성의 수정이나 주어진 이미지의 지역적 편집을 허용하지만, 새로운 맥락에서 주어진 주제의 새로운 표현을 생성하는 것은 불가능했다. 
### 제어 가능한 생성 모델들
생성 모델을 제어하는 다양한 접근법들이 있었지만, 대부분 주제 중심의 프롬프트 기반 이미지 합성에는 적합하지 않았다. 예를 들어, 일부 방법들은 사용자가 제공한 마스크를 가정하여 수정 영역을 제한했다. Inversion 기법들은 맥락을 수정하면서 주제를 보존하는데 사용될 수 있지만, 정체성을 보존하는 주제의 새로운 샘플 생성에는 부족했다.
#### GAN 맥락에서의 개인화
Pivotal Tuning은 역변환된 잠재 코드 앵커로 모델을 미세 조정하여 실제 이미지 편집을 가능하게 했다. 하지만 이러한 방법들은 대략 100장의 이미지가 필요하고 얼굴 도메인에 제한되어 있었다.
# Method
## Text-to-Image Diffusion Models 기초 원리
### Diffusion Model의 수학적 정의
Diffusion Model은 가우시안 분포에서 샘플링한 변수를 점진적으로 denoising하여 데이터 분포를 학습하는 확률적 생성 모델이다. 조건부 확산 모델 $\hat{x}_\theta$​는 다음의 제곱 오차 손실로 학습된다:
$$
\mathbb{E}_{x, c, \epsilon, t} [w_t \| \hat{x}_\theta (\alpha_t x + \sigma_t \epsilon, c) - x \|_2^2]
$$
여기서:
- $x$: 실제 이미지 (ground-truth image)
- $c$: 조건 벡터 (conditioning vector): 텍스트 프롬프트에서 유도
- $\epsilon \sim \mathcal{N}(0,I)$: 노이즈 항
- $\alpha_t, \sigma_t, w_t$: 노이즈 스케줄과 샘플 품질을 제어하는 항들
- $t \sim \mathcal{U}([0,1])$: 확산 프로세스 시간
### 케스케이드 구조
최신 text-to-image 확산 모델들은 고해상도 이미지를 생성하기 위해 케스케이드 구조를 사용한다:
1. $64 \times 64$ 해상도의 기본 text-to-image 모델
2. $64 \times 64 \to 256 \times 256$ 텍스트 조건부 초해상도 모델
3. $256 \times 256 \to 1024 \times 1024$ 초해상도 모델
## 특별한 식별자를 통한 주제 표현
### 프롬프트 설계 전략
DreamBooth의 핵심 아이디어는 새로운(고유 식별자, 주제) 쌍을 확산 모델의 "사전"에 이식하는 것이다. 주어진 이미지 세트에 대한 자세한 이미지 설명을 작성하는 오버헤드를 우회하기 위해, 모든 입력 이미지를 `a [identifier] [class noun]` 형식으로 라벨링한다.
여기서 `[identifier]`는 주제에 연결된 고유 식별자이고, `[class noun]`은 주제의 대략적인 클래스 설명자다. 예를 들어 특정 강아지의 경우 `a [V] dog`와 같은 형식을 사용한다.
### 클래스 설명자의 중요성
클래스 설명자를 문장에 사용하는 것은 클래스의 사전 지식을 고유한 주제에 연결하기 위함이다. 실제로 잘못된 클래스 설명자를 사용하거나 클래스 설명자 없이 사용하면 학습 시간이 증가하고 언어 표류가 발생하여 성능이 저하된다는 것을 확인했다.
### 희귀 토큰 식별자의 필요성
기존 영어 단어(예: "unique", "special")를 사용하는 것은 최적이 아니다. 왜냐하면 모델이 이러한 단어들을 원래 의미에서 분리하고 주제를 참조하도록 다시 연결하는 것을 모두 학습해야 하기 때문이다.
### 회귀 토큰 선택 방법
본 연구의 접근법은 어휘에서 희귀 토큰을 찾은 다음 이를 텍스트 공간으로 역변환하는 것이다. 구체적으로:
1. 어휘에서 희귀 토큰 룩업을 수행하여 희귀 토큰 식별자 시퀀스 $f(\hat{V})$를 얻는다
2. 여기서 $f$는 토크나이저이고, $\hat{V}$는 토큰 $f(\hat{V})$에서 파생된 디코딩된 텍스트다
3. 상대적으로 짧은 시퀀스 $k=\{1,2,3\}$이 잘 작동한다
4. T5-XXL 토크나이저 범위 $\{5000, \cdots, 10000\}$에서 3개 이하의 유니코드 문자에 해당하는 토큰을 사용한다
## 클래스 특정 사전 보조 손실
### 발생하는 주요 문제들
#### 과적합(Overfitting)
입력 이미지 수가 매우 적기 때문에 대형 이미지 생성 모델을 미세 조정하면 주어진 입력 이미지의 맥락과 외형 모두에 과적합될 수 있다. 최대 주제 충실도를 위한 최상의 결과는 모델의 모든 레이어를 미세 조정할 때 달성되지만, 이는 텍스트 임베딩에 조건화된 레이어들의 미세 조정을 포함하여 언어 표류 문제를 야기한다.
#### 언어 표류(Language Drift)
이는 언어 모델에서 관찰된 현상으로, 대규모 텍스트 코퍼스에서 사전 훈련되고 나중에 특정 작업을 위해 미세 조정된 모델이 언어의 구문적, 의미적 지식을 점진적으로 잃는 것이다. 확산 모델에서도 유사한 현상이 나타나는데, 모델이 동일한 클래스의 다른 주제들을 생성하는 방법을 천천히 잊어버린다.
#### 사전 보존 손실 공식
이러한 문제들을 해결하기 위해 자생적 클래스 특정 사전 보존 손실(autogenous class-specific prior preservation loss)을 제안한다. 본질적으로 모델을 자체 생성 샘플로 지도하여 few-shot 미세 조정이 시작된 후 사전 지식을 유지한다. 
구체적으로, 고정된 사전 훈련된 확산 모델에서 초기 랜덤 노이즈 $z_{t_1} \sim \mathcal{N}(0,I)$와 조건 벡터 $c_{pr} := \Gamma(f(\text{"a [class noun]"}))$를 사용하여 데이터 $x_{pr} = \hat{x}(z_{t_1}, c_{pr})$를 생성한다.
전체 손실 함수는:
$$
\mathbb{E}_{x, c, \epsilon, \epsilon', t} [w_t \| \hat{x}_\theta (\alpha_t x + \sigma_t \epsilon, c) - x \|_2^2 + \lambda w_{t'} \| \hat{x}_\theta (\alpha_{t'} x_{pr} + \sigma_{t'} \epsilon', c_{pr}) - x_{pr} \|_2^2 ]
$$
여기서 두 번째 항이 사전 보존 항이며, $\lambda$는 이 항의 상대적 가중치를 제어한다.
#### 학습 파라미터
- 반복 횟수: ~1000회
- 학습률: $10^{-5}$ (Imagen), $5 \times 10^{-6}$ (Stable Diffusion)
- $\lambda = 1$
- 주제 데이터셋 크기: 3 - 5장
- 학습 시간: TPUv4에서 약 5분, NVIDIA A100에서 약 5분
## 개인화된 인스턴스 별 초해상도
### 초해상도 모델의 중요성
Text-to-Image Diffusion Model의 대부분의 시각적 의미를 제어하는 반면, 초해상도 모델들은 사실적인 콘텐츠를 달성하고 주제 인스턴스 세부사항을 보존하는 데 필수적이다. 미세 조정 없이 초해상도 네트워크를 사용하면 생성된 출력에 아티팩트가 포함될 수 있다.
### 낮은 수준 노이즈 증강
기존 훈련 방법과 텍스트 파라미터를 사용하여 주제 인스턴스의 few-shot으로 초해상도 모델을 미세 조정하면 결과가 최적이 아니다. 특히 초해상도 네트워크 훈련에 사용되는 원래 노이즈 증강 수준을 유지하면 환경의 고주파 패턴이 흐려진다.
주제 인스턴스를 충실히 재현하기 위해 $256 \times 256$ 초해상도 모델 미세 조정 중 노이즈 증강 수준을 $10^{-3}$에서 $10^{-5}$로 줄인다. 이 작은 수정으로 주제 인스턴스의 세밀한 디테일을 복구할 수 있다.
# Experiments
## 데이터셋과 평가 방법론
### 데이터셋 구성
30개의 다양한 주제로 구성된 데이터셋을 수집했다. 이는 21개의 물체(가방, 봉제 인형, 선글라스, 만화 캐릭터, 시계, 주전자 등)와 9개의 생물체/반려동물(강아지, 고양이, 새 등)로 구분된다. 
각 주제는 일반적으로 3-5장의 이미지로 구성되며, 다양한 맥락에서 캡처되었다.
### 프롬프트 설계
각 카테고리에 대해 25개의 프롬프트를 수집했다
- 물체의 경우: 20개의 재맥락화 프롬프트 + 5개의 속성 수정 프롬프트
- 생물체의 경우: 10개의 재맥락화 + 10개의 액세서리 + 5개의 속성 수정 프롬프트
### 평가 지표
#### 주제 충실도 (Subject Fidelity)
- **CLIP-I**: CLIP 임베딩 사용한 생성 이미지와 실제 이미지 간의 평균 쌍별 코사인 유사도
- **DINO**: ViT-S/16 DINO 임베딩을 사용한 평균 쌍별 코사인 유사도 (선호되는 지표)
DINO가 선호되는 이유는 자기 지도 학습 훈련 목표가 동일한 클래스의 다른 주제들을 구별하도록 장려하기 때문이다. 반면 CLIP은 지도 학습 네트워크로서 텍스트 주석에 없는 차이점들을 무시하도록 훈련되었다.
#### 프롬프트 충실도 (Prompt Fidelity)
- CLIP-T: 프롬프트와 이미지 CLIP 임베딩 간의 평균 코사인 유사도
## 주요 응용 분야
### 재맥락화 (Recontextualization)
이는 가장 기본적이면서도 강력한 응용이다. 특정 주제를 완전히 다른 환경에 배치하는 것으로, `a [V] [class noun] [context description]` 형식의 프롬프트를 사용한다. 
중요한 점은 단순한 배경 교체가 아니라는 것이다. 생성된 이미지들은 다음과 같은 사실적인 통합 디테일을 포함한다:
- 다른 물체와의 사실적인 접촉 (예: 부분적으로 눈에 묻힌 물체)
- 현실적인 그림자와 반사
- 새로운 포즈와 관절 움직임
- 환경과의 자연스러운 상호작용
![[Pasted image 20250701131524.png]]
### 예술적 표현 (Art Renditions)
`a painting of a [V] [class noun] in the style of [famous painter]` 형식의 프롬프트를 사용한다. 이는 단순한 스타일 전송과는 다르다. 스타일 전송이 소스 구조를 보존하고 스타일만 전송하는 반면, 이 방법은 예술적 스타일에 따라 의미 있는 새로운 변화를 생성하면서도 주제 정체성을 보존한다.
![[Pasted image 20250701131538.png]]
### 새로운 시점 합성 (Novel View Synthesis)
원본 이미지에서 보지 못한 새로운 관점에서 주제를 렌더링한다. 예를 들어, 정면 이미지만 가지고 있던 고양이의 뒷모습, 위에서 본 모습, 아래에서 본 모습 등을 생성할 수 있다. 이는 클래스 사전 지식을 활용하여 보이지 않는 부분을 추론하는 능력을 보여준다.
![[Pasted image 20250701131554.png]]
### 속성 수정 (Property Modification)
- 색상 변경: `a [color] [V] [class noun]` 형식
- 재질 변경: `a transparent [V] teapot` 같은 형식
- 종 혼합: `a cross of a [V] dog and a [target species]` 형식으로 특정 동물과 다른 종을 결합
![[Pasted image 20250701131641.png]]
### 표정 조작과 액세서리
입력 이미지에 없던 다양한 표정(우울한, 기쁜, 놀란, 화난 등)을 생성하거나, 다양한 액세서리(모자, 안경, 의상 등)를 착용한 모습을 생성할 수 있다.
## 비교 연구
### Textual Inversion과의 정량적 비교

| 방법                             | DINO ⬆️ | CLIP-I ⬆️ | CLIP-T ⬆️ |
| ------------------------------ | ------- | --------- | --------- |
| Real Images                    | 0.774   | 0.885     | N/A       |
| DreamBooth (Imagen)            | 0.696   | 0.812     | 0.306     |
| DreamBoooth (Stable Diffusion) | 0.668   | 0.803     | 0.305     |
| Textual Inversion              | 0.569   | 0.780     | 0.255     |
### 사용자 선호도 연구
72명의 사용자를 대상으로 한 연구에서:
- 주제 충실도: 68%가 DreamBooth 선호
- 프롬프트 충실도: 81%가 DreamBooth 선호
- 결정 불가: 10% (주제), 7% (프롬프트)
## 구성요소 분석 (Ablation Studies)
### 클래스 사전 지식의 영향

| 클래스 설정  | DINO ⬆️ | CLIP-I ⬆️ |
| ------- | ------- | --------- |
| 올바른 클래스 | 0.744   | 0.853     |
| 클래스 없음  | 0.303   | 0.607     |
| 잘못된 클래스 | 0.454   | 0.728     |
올바른 클래스명을 사용하는 것이 주제 충실도에 결정적임을 보여준다.
![[Pasted image 20250701131712.png]]
### 사전 보존 손실의 효과

| 방법     | PRES ⬇️ | DIV ⬆️ | DINO ⬆️ | CLIP-I ⬆️ |
| ------ | ------- | ------ | ------- | --------- |
| PPL 있음 | 0.493   | 0.391  | 0.684   | 0.815     |
| PPL 없음 | 0.664   | 0.371  | 0.712   | 0.828     |
여기서 PRES는 사전 보존 지표(낮을수록 좋음), DIV는 다양성 지표(높을수록 좋음)이다. 사전 보존 손실이 언어 표류를 상당히 완화하고 다양성을 유지하는 데 도움이 됨을 보여준다.
![[Pasted image 20250701131724.png]]
![[Pasted image 20250701131745.png]]
### 훈련 이미지 수의 영향

| 이미지 수      | 1     | 2     | 3     | 4     | 5     |
| ---------- | ----- | ----- | ----- | ----- | ----- |
| 가방 (DINO)  | 0.494 | 0.515 | 0.596 | 0.604 | 0.597 |
| 강아지 (DINO) | 0.798 | 0.851 | 0.871 | 0.876 | 0.864 |
일반적으로 4장의 이미지가 최적이지만, 더 일반적인 주제(강아지)는 더 적은 이미지로도 좋은 결과를 얻을 수 있다.
## 한계점과 실패 사례
### 주요 실패 모드
1. **드문 맥락 생성 실패**: 훈련 데이터에서 드물었던 맥락이나 상황을 정확히 생성하지 못할 수 있다
2. **Context-Appearance 얽힘**: 프롬프트된 맥락으로 인해 주제의 외형이 변경되는 경우 (예: 파란 천 위에 있다고 하면 주제도 파란색으로 변함)
3. **과적합**: 프롬프트가 원래 설정과 유사할 때 훈련 세트와 비슷한 이미지만 생성
![[Pasted image 20250701131805.png]]
### 기타 제한사항
- 일부 주제(강아지, 고양이)가 다른 주제보다 학습하기 쉬움
- 희귀하거나 복잡한 주제는 더 많은 변형을 지원하지 못할 수 있음
- 생성된 이미지의 주제 충실도에 변동성이 있음
- 모델 사전의 강도와 의미적 수정의 복잡성에 따라 환각된 주제 특징이 포함될 수 있음
# Conclusions
## 주요 기술적 성과
DreamBooth는 text-to-image 생성 분야에서 패러다임 변화를 가져왔다. 단 몇 장의 이미지(3-5장)만으로 특정 주제의 핵심 시각적 특징을 보존하면서 다양한 맥락에서 새로운 표현을 합성할 수 있는 첫 번째 기술이다. 이는 다음과 같은 핵심 혁신을 통해 달성되었다:
- **희귀 토큰 식별자 전략**: 기존 단어의 의미와 충돌하지 않는 고유 식별자를 사용함으로써 깨끗한 학습이 가능해졌다
- **클래스 특정 사전 보존**: 자생적 사전 보존 손실을 통해 특정성과 일반성을 동시에 유지하는 것이 가능해졌다
- **효율적인 학습 프로세스**: 5분이라는 짧은 학습 시간으로 실용적인 개인화가 가능해졌다
## 응용 가능성과 영향
이 기술은 창작자들에게 전례 없는 도구를 제공한다. 개인 사진에서 전문적인 제품 시각화, 예술적 표현, 마케팅 콘텐츠 제작까지 광범위한 응용이 가능하다. 특히 "magic photo booth" 비유에서 보여주듯이, 복잡한 기술적 지식 없이도 창의적인 콘텐츠를 생성할 수 있는 접근성을 제공한다.
## 미래 연구 방향
이 연구는 여러 흥미로운 연구 방향을 제시한다:
- 더 적은 이미지(1-2)로도 효과적인 학습이 가능한 방법 개발
- 더 복잡하고 다양한 주제들에 대한 확장성 개선
- 실시간 또는 더 빠른 개인화 기술 개발
- 다중 주제를 동시에 학습하고 조합하는 기술
## 윤리적 고려사항과 책임감 있는 사용
연구진들은 이 기술의 잠재적 오용 가능성을 인정하고 있다. 딥페이크 생성, 저작권 침해, 잘못된 정보 유포 등의 문제가 발생할 수 있다. 따라서 기술 발전과 함께 안전장치 개발, 윤리적 가이드라인 수립, 법적 프레임워크 정비 등이 동반되어야 한다.
**결론적으로**, DreamBooth는 개인화된 AI 생성 도구의 시대를 열었으며, 인간의 창의성과 AI 기술이 만나는 새로운 지평을 제시했다. 이는 단순한 기술적 진보를 넘어서 창작의 민주화와 개인화된 콘텐츠 생성의 새로운 패러다임을 만들어냈다는 점에서 그 의의가 크다.